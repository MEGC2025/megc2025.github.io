<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html xmlns="http://www.w3.org/1999/html">
<head>
    <title>ACMMM MEGC2025</title>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
    <link href="assets/css/main.css" rel="stylesheet"/>
</head>
<body class="is-preload">
<div id="page-wrapper">

    <!-- Header -->
    <header id="header">
        <div class="logo container">
            <div>
                <h1><a href="index.html" id="logo">ACM MM MEGC2025 </a></h1>
                <p>Facial Micro-Expression Grand Challenge 2025</p>
            </div>
        </div>
    </header>

    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="index.html">Home</a></li>
<!--            <li><a href="workshop.html">FME'22 Workshop</a>-->
<!--                <ul>-->
<!--                    <li><a href="workshop.html#agenda">Agenda</a></li>-->
<!--                    <li><a href="workshop.html#submission">Submission</a></li>-->
<!--                </ul>-->
<!--            </li>-->
            <!-- MENU -->
            <li><a href="challenge.html">MEGC2025 Challenge</a>
                <ul>
                    <li><a href="challenge.html#news">News</a></li>
                    <li><a href="challenge.html#dates">Important Dates</a></li>
                    <li><a href="challenge.html#unseenSet">Unseen Test Set</a></li>
                    <li><a href="challenge.html#STR">Spot-then-Recognize (STR) Task </a></li>
                    <li><a href="challenge.html#VQA">Visual Question Answering (VQA) Task</a></li>
                    <!-- <li><a href="challenge.html#training">Recommended Training Databases</a></li> -->
                    <li><a href="challenge.html#submission">Submission</a></li>
                    <!-- <li><a href="challenge.html#winners">Top three winners</a></li> -->
                    <!-- <li><a href="challenge.html#questions">Frequently Asked Questions</a></li> -->
                    <!-- <li><a href="challenge.html#winners">Winners</a></li> -->
                </ul>
                </li>
                <li><a href="program.html">Program</a></li>
                <li><a href="organisers.html">Organisers</a></li>
                <li><a href="review.html">Continuity</a></li>
                <!-------------END OF MENU--------------->
        </ul>
    </nav>

    <!-- Main -->
    <section id="main">
        <div class="container">

            <!-- Pre-amble section -->
            <section>
                <!-- Click <a href="files/CfP_MEGC2024_final.pdf">here</a> to download the CFP. -->
                This year's Grand Challenge comprises of two tracks:
                <ul>
                    <li><a href="#STR">Spot-then-Recognize (STR) Challenge</a> | <a href="https://www.codabench.org/competitions/8390/">Codabench site</a>  </li>
                    <li><a href="#VQA">Visual Question Answering (VQA)</a> | <a href="https://www.codabench.org/competitions/8435/">Codabench site</a>  </li>
                </ul>
                <!--
                <p><b>UPDATE</b>: The leaderboard will be closing on 13th July, 9:00, Anywhere on Earth (AoE), and the top 3 entries on the leaderboard will be invited to submit papers to the conference by <b>14th July AoE (the official hard deadline of ACM MM grand challenges)</b>.
                </br>
->For anyone outside of the top 3, we highly encourage you to submit to our sister workshop:  <a
                        href="https://megc2023.github.io/workshop.html">https://megc2023.github.io/workshop.html</a>, the submission system for the Facial Micro-Expression workshop is open here: <a
                             href="https://easychair.org/conferences/?conf=fme2023">https://easychair.org/conferences/?conf=fme2023. </a>   </p>

                <p><b>UPDATE</b>: <del>The deadline for submitting your results to the leaderboard will be the same as the paper submission date -<b> 14th July 2023 AOE</b></del>.
                    After this deadline, the leaderboard rankings will be made public. The best submission score from each participant will be used. </p>

<p><b>NOTICE ON EVALUATION</b>: To facilitate paper reviewing, as the leaderboard scores are hidden, participants could state their results using SAMM-LV, CAS(ME)^2, CAS(ME)^3, and/or the MEGC 2022 unseen dataset at the article submission stage. The use of all these datasets for this purpose is not mandatory, and participants are free to choose a subset or another dataset if they wish. Final scores can be added in the camera-ready version.</p>


                 <p><b>[update!!] Leaderboard's closure has been postponed.</b>

                <p><b>[update]</b> The unseen dataset is available for application now! Click <a
                        href="challenge.html#test">here</a> for more detail.

                <p><b>[update]</b> The LeaderBoard is available for result submission now! Click <a
                        href="challenge.html#Eval">here</a> for more detail.
               -->
                Our challenge overview paper is here: X, Fan, J. Li, J. See,  M. H. Yap, W.-H. Cheng, X. Li, X. Hong, S.-J. Wang and A. K. Davison. <a href="https://arxiv.org/pdf/2506.15298">MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering</a>. arXiv preprint arXiv:2506.15298, 2025. </p>
            </section>

            <!-- News Section -->
            <section>
                <a name="news"></a>
                <h2 class="major"><span>News</span></h2>
                <ul>
                    <li><b>27/06/2025: </b>Updated STR example submission file</li>
                    <li><b>27/06/2025: </b>Updated paper submission information</li>
                    <li><b>27/06/2025: </b> Updated submission deadline for STR track and
                        paper submission </li>
                </ul>
            </section>


			<section id="competition-results" class="results-section">
    <div class="section-header">
        <h2 class="major"><span>Top 3 for MEGC2025</span></h2>
    
    </div>
    
    <div class="task-container">
        <div class="task-card">
            <div class="task-title">
                <h3>ME-VQA Task</h3>
                <span class="task-subtitle">Micro-Expression Visual Question Answering</span>
            </div>
            
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th class="ranking-col">Ranking</th>
                            <th class="title-col">Paper Title</th>
                            <th class="authors-col">Authors</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="first-place">
                            <td class="ranking">1st Place</td>
                            <td class="paper-title">Emotion-Qwen-VL: A fully fine-tuned multimodal large language model for micro-expression visual question answering</td>
                            <td class="authors">Yujing Wang, Ruotong Fang, Xing Huang, Zhiyuan Han, Xiaoqing Lin, Yuhao Shan, Tong Chen</td>
                        </tr>
                        <tr class="second-place">
                            <td class="ranking">2nd Place</td>
                            <td class="paper-title">HierMEQA: A Relationship-Aware Hierarchical Framework for Consistent Micro-Expression Visual Question Answering</td>
                            <td class="authors">Lingsi Zhu, Jun Yu, Yanjun Chi, Gongpeng Zhao, Yuefeng Zou, Fengzhao Sun</td>
                        </tr>
                        <tr class="third-place">
                            <td class="ranking">3rd Place</td>
                            <td class="paper-title">Temporal Information Enhanced Visual-Language Model for Facial Micro-expression Visual Question Answering</td>
                            <td class="authors">Bin Cai, Haofu Yang, Jianxing Wang, Kaiyi Peng, Zhongxiang Cai, Wenbo Zhang, Jinshui Miao</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        
        <div class="task-card">
            <div class="task-title">
                <h3>ME-STR Task</h3>
                <span class="task-subtitle">Micro-Expression Spotting and Recognition</span>
            </div>
            
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th class="ranking-col">Ranking</th>
                            <th class="title-col">Paper Title</th>
                            <th class="authors-col">Authors</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="first-place">
                            <td class="ranking">1st Place</td>
                            <td class="paper-title">Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression</td>
                            <td class="authors">Zizheng Guo, Bochao Zou, Yinuo Jia, Xiangyu Li, Huimin Ma</td>
                        </tr>
                        <tr class="second-place">
                            <td class="ranking">2nd Place</td>
                            <td class="paper-title">FGSL: Frequency-Gated Spatiotemporal Learning for Micro-Expression Spotting-Recognition</td>
                            <td class="authors">Linsi Zhu, Yanjun Chi, Jun Yu, Gongpeng Zhao, Yuefeng Zou, Fengzhao Sun, XiLong Lu</td>
                        </tr>
                        <tr class="third-place">
                            <td class="ranking">3rd Place</td>
                            <td class="paper-title">Spatiotemporal Multimodal Large Language Model for Facial Micro-expression Spot-then-Recognize</td>
                            <td class="authors">Dangen Li, Yuqun Ma, Hang Pan</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>
    
    <style>
        .results-section {
            max-width: 1200px;
            margin: 40px auto;
            padding: 20px;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            color: #333;
        }
        
        .section-header {
            text-align: center;
            margin-bottom: 40px;
        }
        
        .section-header h2 {
            font-size: 2.2rem;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        
        .section-header p {
            font-size: 1.1rem;
            color: #7f8c8d;
        }
        
        .task-container {
            display: flex;
            flex-direction: column;
            gap: 40px;
        }
        
        .task-card {
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .task-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.12);
        }
        
        .task-title {
            background: linear-gradient(135deg, #3498db, #2c3e50);
            color: white;
            padding: 20px 25px;
        }
        
        .task-title h3 {
            font-size: 1.6rem;
            margin-bottom: 5px;
        }
        
        .task-subtitle {
            font-style: italic;
            opacity: 0.9;
        }
        
        .table-container {
            overflow-x: auto;
        }
        
        .results-table {
            width: 100%;
            border-collapse: collapse;
        }
        
        .results-table th {
            background-color: #f8f9fa;
            color: #2c3e50;
            padding: 15px;
            text-align: left;
            font-weight: 600;
            border-bottom: 2px solid #eaeaea;
        }
        
        .results-table td {
            padding: 15px;
            border-bottom: 1px solid #eaeaea;
        }
        
        .ranking-col {
            width: 120px;
        }
        
        .title-col {
            width: 45%;
        }
        
        .authors-col {
            width: 40%;
        }
        
        .ranking {
            font-weight: bold;
            text-align: center;
            padding: 8px 12px;
            border-radius: 20px;
            display: inline-block;
            min-width: 90px;
        }
        
        .first-place .ranking {
            background-color: rgba(231, 76, 60, 0.1);
            color: #e74c3c;
        }
        
        .second-place .ranking {
            background-color: rgba(243, 156, 18, 0.1);
            color: #f39c12;
        }
        
        .third-place .ranking {
            background-color: rgba(46, 204, 113, 0.1);
            color: #2ecc71;
        }
        
        .paper-title {
            font-weight: 600;
            color: #2c3e50;
        }
        
        .authors {
            color: #555;
            line-height: 1.5;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        tr:hover {
            background-color: #e8f4fc;
        }
        
        @media (max-width: 768px) {
            .results-section {
                padding: 15px;
            }
            
            .section-header h2 {
                font-size: 1.8rem;
            }
            
            .task-title {
                padding: 15px 20px;
            }
            
            .task-title h3 {
                font-size: 1.4rem;
            }
            
            .results-table th,
            .results-table td {
                padding: 10px;
                font-size: 0.9rem;
            }
            
            .ranking-col {
                width: 100px;
            }
        }
        
        @media (max-width: 480px) {
            .ranking {
                min-width: 70px;
                font-size: 0.85rem;
                padding: 6px 8px;
            }
            
            .results-table th,
            .results-table td {
                padding: 8px;
                font-size: 0.85rem;
            }
        }
    </style>
</section>
            

            <!-- Important Dates Section -->
            <section>
                <a name="dates"></a>
                <h2 class="major"><span>Important Dates</span></h2>
                <ul>
                    <li><a href="challenge.html#unseenSet"><b>Test Set Release: </b></a>
                        19th May 2025 </li>   
                    <li><b>Challenge Platform Submission Open: </b>
                        23rd May 2025 </li>    
                    <li><b>STR Track Challenge Submission Deadline (for Codabench only): </b>
                        <s>27th June 2025</s> 4th July 2025 at 23:59 AoE <font color="red">(FIRM)</font></li>
                        <li><b>VQA Track Challenge Submission Deadline (for Codabench only): </b>
                        27th June 2025 at 11:59 AoE <font color="red">(FIRM)</font></li>
                    <li><b>Paper Invitation</b> (after confirmation of results): 
                        <s>3rd July 2025</s> 7th July 2025 </li>
                    <li><b>Paper Submission deadline: </b>
                        <s>30th July 2025</s> 31st July 2025</li>
                    <li><b>Notification of Accepted Papers: </b>
                        <s>7th August 2025</s> 8th August 2025 </li>
                    <li><b>Camera-Ready Deadline: </b>
                        26th August 2025 </li>
                    </ul>                                   
            </section>

		 <!-- Unseen dataset-->
            <section>
                <a name="unseenSet"></a>
                <header class="main">
                    <h2 class="major"><span>Unseen dataset for both tasks</span></h2>
                </header>

                <p>
                    This year, we will be using the unseen cross-cultural test sets to evaluate algorithms' performances in a fairer manner.
                </p>
                <h3>Unseen Dataset for STR</h3>
		        <ul>
                    <li>The unseen testing set (<code><b>MEGC2025-testSet</b></code>) (same version as MEGC2023 Unseen dataset)
                        contains 30 long video, including 10 long videos from SAMM (SAMM Challenge dataset) and 20 clips cropped
                        from different videos in CAS(ME)<sup>3</sup> (unreleased before). The frame rate for SAMM Challenge dataset
                        is 200fps and the frame rate for CAS(ME)<sup>3</sup> is 30 fps. The participants should test on this unseen dataset.
                    <LI>To obtain the <code><b>MEGC2025-testSet</b></code>, download and fill in the <a
                            href="files/SAMM_ReleaseAgreementV2.pdf">license agreement form of SAMM Challenge
                        dataset</a> and
                        the <a href="files/CAS3_clip_ReleaseAgreement.pdf">license agreement form of CAS(ME)<sup>3</sup>_clip</a>,
                        upload the file through this
                        link: <a href="https://www.wjx.top/vm/wxCeVHP.aspx#">https://www.wjx.top/vm/wxCeVHP.aspx#  </a>.

                        <uL>
                            <li> For the request from a bank or company, the participants are required to ask their director or
                                CEO to sign the form.
                            </li>
                            <li> Reference:
                                <ol>
                                    <li> Li, J., Dong, Z., Lu, S., Wang, S.J., Yan, W.J., Ma, Y., Liu, Y., Huang, C. and
                                        Fu, X. (2023). CAS(ME)<sup>3</sup>: A Third Generation Facial Spontaneous
                                        Micro-Expression Database with Depth Information and High Ecological Validity.
                                        <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, vol. 45, no. 3, pp. 2782-2800, 1 March 2023, doi: 10.1109/TPAMI.2022.3174895.
                                    </li>
                                    <li> Davison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A
                                        spontaneous micro-facial movement dataset. <i>IEEE Transactions on Affective
                                        Computing</i>, 9(1),
                                        116-129.
                                    </li>
                                </ol>
                            </li>
                        </ul>
                    </li>
                </ul>
                <h3>Unseen Dataset for VQA</h3>
                <ul>
                    <li>The unseen testing set for VQA contains 24 ME clips, including 7 clips from SAMM (SAMM Challenge dataset) and 17 clips
                        from different videos in CAS(ME)<sup>3</sup> (unreleased before). The frame rate for SAMM Challenge dataset
                        is 200fps and the frame rate for CAS(ME)<sup>3</sup> is 30 fps. The participants should test on this unseen dataset.
                    <LI>To obtain the <code><b>MEGC2025-testSet-ME-VQA</b></code>, download and fill in the <a
                            href="files/SAMM_ReleaseAgreementV2.pdf">license agreement form of SAMM Challenge
                        dataset</a> and
                        the <a href="files/CAS3_clip_ReleaseAgreement.pdf">license agreement form of CAS(ME)<sup>3</sup>_clip</a>,
                        upload the file through this
                        link: <a href="https://www.wjx.top/vm/wxCeVHP.aspx#">https://www.wjx.top/vm/wxCeVHP.aspx#  </a>.

                        <uL>
                            <li> For the request from a bank or company, the participants are required to ask their director or
                                CEO to sign the form.
                            </li>
                            <li> Reference:
                                <ol>
                                    <li> Li, J., Dong, Z., Lu, S., Wang, S.J., Yan, W.J., Ma, Y., Liu, Y., Huang, C. and
                                        Fu, X. (2023). CAS(ME)<sup>3</sup>: A Third Generation Facial Spontaneous
                                        Micro-Expression Database with Depth Information and High Ecological Validity.
                                        <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, vol. 45, no. 3, pp. 2782-2800, 1 March 2023, doi: 10.1109/TPAMI.2022.3174895.
                                    </li>
                                    <li> Davison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A
                                        spontaneous micro-facial movement dataset. <i>IEEE Transactions on Affective
                                        Computing</i>, 9(1),
                                        116-129.
                                    </li>
                                </ol>
                            </li>
                        </ul>
                    </li>
                </ul>
            
            <!-- Track 1 STR Section -->
            <section>
                <a name="STR"></a>
                <header class="main">
                    <h2 class="major"><span>Spot-then-Recognize (STR) Task</span></h2>
                </header>

                <p>Since the rapid advancement of ME research started about a decade ago, most works have been mainly focused on
                    two separate tasks: spotting and recognition. The task of only recognizing the ME class can be unrealistic in
                    real-world settings since it assumes that the ME sequence has already been identified - an ill-posed problem in
                    the case of a continuous-running video. On the other hand, the spotting task is unrealistic in its applicability
                    since it cannot interpret the actual emotional state of the person observed.
                </p>
                    
                <p>A more realistic setting, also known as "spot-then-recognize", performs spotting followed by recognition in a
                    sequential manner. Only samples that have been correctly spotted in the spotting step (i.e. true positives)
                    will be passed on to the recognition step to be classified for its emotion class.
                    The task will use the unseen dataset, and evaluated using <a href="#STR-eval">selected metrics</a>.</p> 

                Reference:
                    <ul>
                        <li>Liong, G-B., See, J. and C.S. Chan (2023). Spot-then-recognize: A micro-expression analysis network for seamless evaluation
                            of long videos.
                         <i>Signal Processing: Image Communication</i>, vol. 110, pp. 116875, January 2023, doi: <a href="https://doi.org/10.1016/j.image.2022.116875">10.1016/j.image.2022.116875</a>
                         </li>
                     </ul>

                <p></p>
                <a name="STR-eval"></a>
                <h3>Evaluation Protocol</h3>
                <ul>
                    <li>Submissions will use the Codabench Competition Leaderboard.</li>                   
                    <li>Participants should upload the predicted results for both the unseen CAS(ME)<sup>3</sup> and SAMM datasets to the
                        Codabench Leaderboard where specific evaluation metrics will be calculated.</li>
                    <li><b>Evaluation metrics</b> (for SAMM, CAS): 
                        <ul>
                            <li>F1-score, for Spotting and Analysis steps. <em>(Higher the better)</em></li>
                            <li>Spot-then-Recognize Score (STRS), which is the product of the Spotting and Analysis F1-scores. <em>(Higher the better)</em></li>
                        </ul>
                    </li>
                    <li>Submissions to the Leaderboard must be made in the form of a <b>zip</b> file containining the predicted csv files with the following filenames:<br />
                        <ul>
                            <li><code>cas_pred.csv</code> (for the CAS(ME)<sup>3</sup> samples)</li>
                            <li><code>samm_pred.csv</code> (for the SAMM samples)</li>
                        </ul>
                    </li>
                    <li>An example submission is provided here: <a href="files/example_submission_STR.zip">example_submission_STR</a>.</li>
                    <li>The evaluation script is available at <a href="https://github.com/genbing99/STRS-Metric">https://github.com/genbing99/STRS-Metric</a>.</li>
                    <li>The <b>baseline</b> method can be found in the following paper (please cite): 
                        <br/>
                        Liong, G-B., See, J. and C.S. Chan (2023). Spot-then-recognize: A micro-expression analysis network for seamless evaluation
                        of long videos. Signal Processing: Image Communication, Vol. 110, pp. 116875.
                    </li>        
                </ul>

                <h3>Recommended Training Databases</h3>  <a name="training"></a>
                <uL>
                    <LI><b>SAMM Long Videos with 147 long videos at 200 fps (average duration: 35.5s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                                Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                                with email subject: SAMM long videos.
                            </li>
                            <li>
                                Reference: Yap, C. H., Kendrick, C., & Yap, M. H. (2020, November). SAMM long videos: A
                                spontaneous facial micro-and macro-expressions dataset. In 2020 15th IEEE International
                                Conference on Automatic Face and Gesture Recognition (FG 2020) (pp. 771-776). IEEE.
                            </li>
                        </uL>

                    <LI><b> CAS(ME)<sup>2</sup> with 97 long videos at 30 fps (average duration: 148s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e3">http://casme.psych.ac.cn/casme/e3</a>.  Download and fill in the license agreement form, submit throuth the website.
                            >.
                            </li>
                            <li>
                                Reference: Qu, F., Wang, S. J., Yan, W. J., Li, H., Wu, S., & Fu, X. (2017). CAS (ME) $^
                                2$: a database for spontaneous macro-expression and micro-expression spotting and
                                recognition. IEEE Transactions on Affective Computing, 9(4), 424-436.
                            </li>

                        </uL>

                    <LI><b>SMIC-E-long with 162 long videos at 100 fps (average duration: 22s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Tran, T. K., Vo, Q. N., Hong, X., Li, X., & Zhao, G. (2021). Micro-expression
                                spotting: A new benchmark. Neurocomputing, 443, 356-368.
                            </li>
                        </uL>
                    <LI><b> CAS(ME)<sup>3</sup> with 1300 long videos at 30 fps (average duration: 98s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e4">http://casme.psych.ac.cn/casme/e4</a>.
                                Download and fill in the license agreement form, submit throuth the website.
                            </li>
                            <li>
                                Reference: Li, J., Dong, Z., Lu, S., Wang, S. J., Yan, W. J., Ma, Y., ... & Fu, X. (2022). CAS (ME)<sup>3</sup>: A third generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 2782-2800, doi: 10.1109/TPAMI.2022.3174895..
                            </li>

                        </uL>
                    <LI><b> 4DME with 270 long videos at 60 fps (average duration: 2.5s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis">https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis</a>.
                                Download and fill in the license agreement form , email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Li, X., Cheng, S., Li, Y., Behzad, M., Shen, J., Zafeiriou, S., ... & Zhao, G. (2022). 4DME: A spontaneous 4D micro-expression dataset with multimodalities. IEEE Transactions on Affective Computing.
                            </li>
                        </ul>
                </ul>
                <p></p>

                <!--
                <h3>Test Prompts</h3>  <a name="test"></a>
                -->

            </section>

            <!-- Track 2 VQA Section -->
            <section>
                <a name="VQA"></a>
                <header class="main">
                    <h2 class="major"><span>Visual Question Answering (VQA) Task</span></h2>
                </header>

                <p>For the first time in MEGC 2025, this is a new task that introduces a visual question answering challenge for ME analysis, to leverage on advanced vision-language models (VLMs) and multimodal large language models (LLMs). Instead of relying on structured labels, ME annotations, such as emotion classes, action units, are converted into question-answer (QA) pairs. Given an image or video sequence as input with natural language prompts, models must generate answers that describe the ME, its attributes, etc. These questions can cover a wide range of attributes, from binary classification such as <i>"Is the action unit lip corner depressor shown on the face?"</i> to multiclass classification like <i>"What is the expression class?"</i>, and to more complex inquiries like <i>"What are the action units present, and based on them, what is the expression class?"</i></p>

                <p>Participants may train the model based on the <a href="https://megc2025.github.io/files/me_vqa_samm_casme2_smic.jsonl">curated ME VQA datasets</a> or explore zero-shot reasoning, in-context learning, multi-agent systems, etc. Evaluation will assess the UF1 and UAR of the emotion classes, and NLP metrics BLEU and ROUGE of overall responses. In addition to automatic metrics, human evaluation may be used to gauge the reasoning quality of the models. This task provides a new multimodal perspective on ME analysis, encouraging interpretable and context-aware ME analysis through natural language interaction. 
		
		        The curated ME VQA dataset is improved from the MEGC2019 composite dataset with clips from CASME II, SAMM, and SMIC by adding QA pairs. The participant can use it as a starting point, while they can also include other training samples and generate their own QA pairs.
                </p>
                
                <p>We produce the baseline results in our challenge paper: X, Fan, J. Li, J. See,  M. H. Yap, W.-H. Cheng, X. Li, X. Hong, S.-J. Wang and A. K. Davison. <a href="https://arxiv.org/pdf/2506.15298">MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering</a>. arXiv preprint arXiv:2506.15298, 2025. </p>

                <a name="VQA-eval"></a>
                <h3>Evaluation Protocol</h3>
                <ul>
                    <li>Submissions will use the Codabench Competition Leaderboard.</li>                   
                    <li>Participants should upload the predicted results for both unseen CAS(ME)<sup>3</sup> and SAMM datasets to the
                        Codabench Leaderboard where specific evaluation metrics will be calculated.</li>
                    <li><b>Evaluation metrics</b> (for SAMM, CAS): 
                        <ul>
                            <li>UF1 and UAR for both coarse and fine-grained emotion class. <em>(Higher the better)</em></li>
                            <li>BLEU and ROUGH-1 for all the answers. <em>(Higher the better)</em></li>
                        </ul>
                    </li>
                    <li>Participants can fill in the test VQA to-answer josonl files and renamed them as xxx_pred.jsonl. These files are available at <a href="https://drive.google.com/drive/folders/1_t1cNv2DMFr43wY5WTng8U1hEICgbJIC?usp=sharing">Google Drive</a> and <a href="https://pan.baidu.com/s/1pKuFTcDgBGVKDzr0wAqNYA?pwd=p3uj">Baidu Drive</a>. <br />
                        <ul>
                            <li><code>me_vqa_casme3_test_to_answer.jsonl</code></li>
                            <li><code>me_vqa_samm_test_to_answer.jsonl</code></li>
                        </ul>
                    </li>
                    <li>Submissions to the Leaderboard must be made in the form of a <b>zip</b> file containining the predicted
                        jsonl files with the following filenames:<br />
                        <ul>
                            <li><code>me_vqa_casme3_test_pred.jsonl</code> (for the unseen CAS(ME)<sup>3</sup> ME clips)</li>
                            <li><code>me_vqa_samm_test_pred.jsonl</code> (for the unseen SAMM ME clips)</li>
                        </ul>
                    </li>
                </ul>
                
                <h3>Recommended Training Databases</h3>  <a name="training"></a>
                <ul>
                    <li><b>Curated ME VQA dataset</b>
                        <ul>
                            <li> Please download the annotation from here <a href="https://megc2025.github.io/files/me_vqa_samm_casme2_smic.jsonl">Curated ME
                                VQA dataset</a>.
                            </li>
                            <li> The Curated ME VQA dataset is improved from the MEGC2019 composite dataset with clips from SAMM, CASME II, and
                                SMIC by adding QA pairs. Therefore, to access the ME clips, please follow the dataset request link below.
                            </li>
                        </ul>
                    <li><b>SAMM with 159 ME clips at 100 fps.</b>
                        <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                                Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                                with email subject: SAMM videos.
                            </li>
                            <li>
                                Reference: Dvison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A spontaneous micro facial movement
                                dataset. IEEE Transactions on Affective Computing, 9(1), 116-129.
                            </li>
                        </uL>

                    <li><b> CASME II with 247 ME clips at 200 fps.</b>
                        <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e3">http://casme.psych.ac.cn/casme/e3</a>. Download and fill in the license agreement form, submit throuth the website.
                            >.
                            </li>
                            <li>
                                Reference: Yan, W. J., Li, X., Wang, S. J., Zhao, G., Liu, Y. J., Chen, Y. H., & Fu, X. (2014). CASME II: An improved
                                spontaneous micro-expression database and the baseline evaluation. PloS one, 9(1), e86041.
                            </li>
                        </ul>

                    <li><b>SMIC-E-long with 162 ME clips at 100 fps (average duration: 22s).</b>
                        <ul>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Tran, T. K., Vo, Q. N., Hong, X., Li, X., & Zhao, G. (2021). Micro-expression
                                spotting: A new benchmark. Neurocomputing, 443, 356-368.
                            </li>
                        </ul>
                    <li><b> CAS(ME)<sup>3</sup> with 1109 ME clips at 30 fps (average duration: 98s).</b>
                        <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e4">http://casme.psych.ac.cn/casme/e4</a>.
                                Download and fill in the license agreement form, submit throuth the website.
                            </li>
                            <li>
                                Reference: Li, J., Dong, Z., Lu, S., Wang, S. J., Yan, W. J., Ma, Y., ... & Fu, X. (2022). CAS (ME)<sup>3</sup>: A third
                                generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions
                                on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 2782-2800, doi: 10.1109/TPAMI.2022.3174895.
                            </li>

                        </ul>
                    <li><b> 4DME with 1068 ME clips at 60 fps (average duration: 2.5s).</b>
                        <ul>
                            <li> To download the dataset, please visit: <a
                                    href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis">https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis</a>.
                                Download and fill in the license agreement form , email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Li, X., Cheng, S., Li, Y., Behzad, M., Shen, J., Zafeiriou, S., ... & Zhao, G. (2022). 4DME: A spontaneous
                                4D micro-expression dataset with multimodalities. IEEE Transactions on Affective Computing.
                            </li>
                        </ul>
                </uL>
                <p></p>

                <!--
                <h3>Test Prompts</h3>  <a name="test"></a>
                -->

            </section>
	    
            <!-- Submission Section -->
            <section>
                <a name="submission"></a>
                <h2 class="major"><span>Submission</span></h2>
                <p>
                    The top 3 papers after the review process will be submitted for publication in the ACM MM'25 proceedings.
                </p>
                <p> Please note: All relevant submission deadlines are at 23:59 <a
                    href="https://www.timeanddate.com/time/zones/aoe">AoE</a> and paper submissions will follow guidelines as per ACM.</p>
                <ul>
                    <li><b>Challenge submission platform for STR task: <a href="https://www.codabench.org/competitions/8390/">Codabench site</a></b></ul></li> 
                    <li><b>Challenge submission platform for VQA task: <a href="https://www.codabench.org/competitions/8435/">Codabench site</a></b></ul></li>
                    <li><b>Submission platform link: TBC</b></li>
 
                        <ul>
                            <li>Submitted papers (.pdf format) must use the ACM Article Template <a
                                href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
                                as used by regular ACM MM submissions. Please use the template in traditional <b>double-column
                                format</b> to prepare your submissions. For example, word users may use Word Interim
                                Template, and latex users may use <b>sample-sigconf</b> template.
                            <li>Grand challenge papers will go through a single-blind review process. Each grand
                                challenge paper submission is limited to 6 pages with 2 extra pages for references only.
                            <li>For all other required files besides the paper, please submit in a
                                single zip file and upload to the submission system as supplementary material. It is compulsory to include:
                                <ul>
                                    <li>GitHub repository URL containing codes of your implemented method, and all other
                                        relevant files such as feature/parameter data.</li>
                                    <li>CSV files reporting the results, i.e.
                                        <code>cas_pred.csv</code>, <code>samm_pred.csv</code> (for both STR and VQA tracks)</li>
                                </ul>
                                The organizers have the right to reject any submissions that: 1) are not accompanied by a paper,
                                2) did not share the code repository and reported results for verification purposes. 
                            </li>
                        </ul>

                
                </ul>
            </section>

    <!-- WINNERS --> 
     <!--        
            <section>
                 <h2 class="major"><span>Winners</span></h2><a name="winners"></a>

                <p>This year, fifteen teams participated in Track 1 (CCS) while four teams participated in Track 2 (STR). The top two submissions with the highest score 
                    (i.e. Overall F1-score for CCS, STRS for STR) were accepted for publication in the MM'24 ACM Proceedings, after a review process was conducted.
                </p>

                 <h3>Track 1: Cross-Cultural Spotting (CCS) Task</h3>
                 
                <style type="text/css">
                        .tg {
                            border-collapse: collapse;
                            border-spacing: 0;
                        }

                        .tg td {
                            border-color: black;
                            border-style: solid;
                            border-width: 1px;
                            font-family: Arial, sans-serif;
                            font-size: 14px;
                            overflow: hidden;
                            padding: 10px 5px;
                            word-break: normal;
                        }

                        .tg th {
                            border-color: black;
                            border-style: solid;
                            border-width: 1px;
                            font-family: Arial, sans-serif;
                            font-size: 14px;
                            font-weight: normal;
                            overflow: hidden;
                            padding: 10px 5px;
                            word-break: normal;
                        }

                        .tg .tg-8sni {
                            border-color: #333333;
                            text-align: center;
                            vertical-align: bottom
                        }

                        .tg .tg-j2c8 {
                            background-color: #f8ff00;
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: bottom
                        }

                        .tg .tg-9ydz {
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: top
                        }

                        .tg .tg-w3y9 {
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: bottom
                        }
                    </style> 
                    
                 <table class = "tg">
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Contestant</th>
                        <th>Affiliation</th>
                        <th>Article Title</th>
                        <th>GitHub Link</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1st Place</strong> 🥇</td>
                        <td>Jun Yu<sup>1</sup>, Yaohui Zhang<sup>1</sup>, Gongpeng Zhao<sup>1</sup>, Peng He<sup>1</sup>, Zerui Zhang<sup>1</sup>, Zhongpeng Cai<sup>1</sup>, Qingsong Liu<sup>2</sup>, Jianqing Sun<sup>2</sup>, and Jiaen Liang<sup>2</sup>
                </td>
                        <td class = "cell">
                        <div> 1 University of Science and Technology of China (USTC)
                </div>
                        <div>2 Unisound AI Technology Co. Ltd.
                </div>

                </td>
                    <td><b>Micro-expression Spotting based on Optical Flow Feature with Boundary Calibration</b>
                </td>
                    <td><a href="https://github.com/new11-ops/2024">https://github.com/new11-ops/2024</a></td>
                    </tr>
                    <tr>
                        <td><strong>2nd Place</strong> 🥈</td>
                    <td>Zhengye Zhang, Sirui Zhao, Xinglong Mao, Shifeng Liu, Hao Wang, Tong Xu, and Enhong Chen
                </td>
                    <td >
                        University of Science and Technology of China (USTC)

                </td>
                    <td><b>A Multi-scale Feature Learning Network with Optical Flow Correction for Micro- and Macro-expression Spotting</b>

                </td>
                    <td><a href="https://github.com/zzy188zzy/megc_spotting_code">https://github.com/zzy188zzy/megc_spotting_code</a></td>
                    </tr>
            </tbody>
            </table>

            <h3>Track 2: Spot-then-Recognize (STR) Task</h3>

            <table class = "tg">
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Contestant</th>
                        <th>Affiliation</th>
                        <th>Article Title</th>
                        <th>GitHub Link</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1st Place</strong> 🥇</td>
                        <td>Jun Yu<sup>1</sup>, Gongpeng Zhao<sup>1</sup>, Yaohui Zhang<sup>1</sup>, Peng He<sup>1</sup>, Zerui Zhang<sup>1</sup>, Zhao Yang<sup>2</sup>, Qingsong Liu<sup>3</sup>, Jianqing Sun<sup>3</sup>, and Jiaen Liang<sup>3</sup>
                </td>
                        <td class = "cell">
                        <div> 1 University of Science and Technology of China (USTC)
                </div>
                        <div>2 Xi'an Jiaotong University, China
                </div>
                        <div>3 Unisound AI Technology Co. Ltd.
                </div>

                </td>
                    <td><b>Temporal-Informative Adapters in VideoMAE V2 and Multi-Scale Feature Fusion for Micro-Expression Spotting-then-Recognize</b>
                </td>
                    <td><a href="https://github.com/zgp123-wq/MEGC2024-STR">https://github.com/zgp123-wq/MEGC2024-STR</a></td>
                    </tr>

                    <tr>
                    <td><strong>2nd Place</strong> 🥈</td>
                    <td>Yuhong He, Wenchao Liu, Guangyu Wang, Lin Ma, and Haifeng Li
                </td>
                    <td >
                        Harbin Institute of Technology, China
                </td>
                    <td><b>Enhancing Micro-Expression Analysis Performance by Effectively Addressing Data Imbalance</b>

                </td>
                    <td><a href="https://github.com/hitheyuhong/MEGC2024-CODE">https://github.com/hitheyuhong/MEGC2024-CODE</a></td>
                    </tr>

                </tbody>
            </table>

            </section>
            -->

            <section>
                <h2 class="major"><span>Frequently Asked Questions</span></h2>
                <a name="questions"></a>
                <ol>
                    <li>Q: How to deal with the spotted intervals with overlap? <br/>
                        A: We consider that each ground-truth interval corresponds to at most one single spotted
                        interval. If your algorithm detects multiple with overlap, you should merge them into an optimal
                        interval. The fusion method is also part of your algorithm, and the final result evaluation only
                        cares about the optimal interval obtained.
                    </li>

                    <li>Q: For the STR challenge, how many classes are used in the classification part? <br/>
                        A: You are required to only classify emotions into three classes: <code>"negative"</code>, <code>"positive"</code>, <code>"surprise"</code>. 
                        Only correctly spotted micro-expressions are passed on to the classification part, also knowns as Analysis (on the Leaderboard).
                        The <code>"other"</code> class is <b>not included</b> in the evaluation calculation for the Analysis part. However, all occurrences, including those labelled 
                        with the <code>"other"</code> class are considered in the Spotting part as they are micro-expressions.
                    </li>
                </ol>

                <br/>
            </section>
        </div>
    </section>
    <footer id="footer">
        <!-- Copyright -->
        <div id="copyright">
            <ul class="menu">
                <li>GET IN TOUCH: a.davison@mmu.ac.uk | x.fan@mmu.ac.uk | j.see@hw.ac.uk | lijt@psych.ac.cn</li>
                <li>&copy; MEGC2025. All rights reserved</li>
                <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
        </div>
    </footer>
</div>

<!-- Scripts -->
<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.dropotron.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
