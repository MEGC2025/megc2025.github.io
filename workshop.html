<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>ACMMM FME'23</title>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
    <link href="assets/css/main.css" rel="stylesheet"/>
</head>
<body class="is-preload">
<div id="page-wrapper">

    <!-- Header -->
    <header id="header">
        <div class="logo container">
            <div>
                <h1><a href="index.html" id="logo">ACMMM FME'23 </a></h1>
                <p>3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression
                    Analysis</p>
            </div>
        </div>
    </header>

    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="index.html">Home</a></li>
<!--            <li class="current"><a href="workshop.html">FME'22 Workshop</a>-->
<!--                <ul>-->
<!--                    <li><a href="workshop.html#agenda">Agenda</a></li>-->
<!--                    <li><a href="workshop.html#submission">Submission</a></li>-->
<!--                </ul>-->
<!--            </li>-->
            <li><a href="challenge.html">MEGC2023 Challenge</a>
                <ul>

                    <li><a href="challenge.html#training">Recommended Training Databases</a></li>
                    <li><a href="challenge.html#test">Unseen Test Dataset</a></li>
                    <li><a href="challenge.html#Eval">Evaluation Protocol</a></li>
                    <li><a href="challenge.html#submission">Submission</a></li>
                    <li><a href="challenge.html#winners">Top three winners</a></li>
                    <li><a href="challenge.html#questions">Frequently Asked Questions</a></li>

                </ul>
            </li>
            <li><a href="workshop.html">FME2023 Workshop</a></li>
<!--            <li><a href="program.html">Program</a></li>-->

            <li><a href="organisers.html">Organisers</a></li>

            <li><a href="review.html">Continuity</a></li>
        </ul>
    </nav>

    <!-- Main -->
    <section id="main">
        <div class="container">

            <!-- Content -->
            <section>
                <p>UPDATE</p>
                <uL>
                    <Li> Join the workshop <a href = "https://teams.microsoft.com/l/meetup-join/19%3ameeting_MTBiMzE1MmUtYmM1Mi00MzVjLTllNTItNTg3N2RmOTI4NDgx%40thread.v2/0?context=%7b%22Tid%22%3a%22283ffb50-a30b-488c-90f4-cdae4f7ae6d1%22%2c%22Oid%22%3a%2223ff0761-4688-4e03-bdb4-3c51cc13b19b%22%7d"><b>online</b></a> on  November 2 from 08:30 -  12:00am (Canadian time,EDT)!
                    <UL>
                        <li> Due to the many visa issues that have occurred at this year’s ACM Multimedia, many of our participants for the grand challenge and our workshop cannot present in-person. Dr. Adrian K. Davison will be introducing the workshop, and allowing any presenters to present if they are available, otherwise a video that has been prepared for the participant will be shown.
                        </li>
                        <li> We will also attempt to facilitate some interactive elements to the workshop to encourage discussion and improvements for future workshops and challenges.



                    </UL>
                </Li>
                <Li> The submission system for the Facial Micro-Expression workshop is open here: <a
                             href="https://easychair.org/conferences/?conf=fme2023">https://easychair.org/conferences/?conf=fme2023. </a>
                <Li> Sumission deadline: <del>21st July 2023 AOE</del> <b>28th July 2023 AOE</b>
                </Li>
                </uL>
            </section>>

            <section>

                <header class="main">
                    <h2 align="center">Facial Micro-Expression (FME) Workshop 2023</h2>
                    <h2 align="center"> - Advanced Techniques for Multi-Modal Facial Expression Analysis</h2>
                    <p align="center">Click <a href="files/CfP_FME_workshop.pdf">here</a> to download the CFP.
                </header>
                <p>
                    Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress the facial expression, typically found in a high-stakes environment. The duration of MEs are very short, generally lasting no more than 500 milliseconds (ms) and is the telltale sign that distinguishes them from a normal facial expression.
Computational analysis and automation of tasks on MEs are emerging areas in face research, with a strong interest appearing as recently as 2014. The availability of a few spontaneously induced facial micro-expression datasets has provided the impetus to further advance in the computational aspect. Since the elicitation and the artificial annotation of MEs are challenging, the amount of labeled ME samples is limited. So far, there are only around 1162 (video) samples across seven public spontaneous databases. Besides, it is impossible to unify the standardization of ME labeling for different annotators. To tackle this problem, we expect that the recent advancement in pattern recognition can help improve ME spotting and recognition performance. For instance, self-supervised learning, one-shot learning, and artificially generating data to aid with the relatively low number of samples.
</p>
                  <p>
                    Furthermore, micro-expression analysis (MEA) faces many challenges. First, the micro-expression generation mechanism is still not precise, while the valence of micro-expression in lie detection is not sufficiently clear. Second, micro-expression samples with high ecological validity are difficult to induce, and data labeling can be quite time-consuming and labor-intensive. This causes the problems of small sample size and imbalanced distribution in MEA tasks.
With the development of imaging devices, MEA is not limited to traditional RGB video. New data and research trends are towards combining facial data captured from multiple and various sensors, e.g., depth and thermal cameras, so that different features can be fused for MEA. Furthermore, MEA is an interdisciplinary field with multi-modality research capabilities. First, multi-modality data, such as depth information and physiological signals, can improve micro-expression analysis performance. Second, multi-modal micro-expression analysis can enable more in-depth research on face and emotion analysis.


                </p>
            </section>

            <section>
                <header class="main">
                    <h2 class="major"><span>Agenda</span></h2>
                    <a name="agenda"></a>
                </header>
        <uL>

                <LI><b>To organize a Facial Micro-Expression (FME) Workshop for facial micro-expression research, involving FME recognition, spotting and generation.</b> </LI>

               <LI><b>To solicit original works that address a variety of challenges of Facial Expressions research, but
                    not limited to:</b>
                    <UL>
                        <li>Facial expressions (both micro- and macro-expressions) detection/spotting</li>
                        <li>Facial expressions recognition</li>
                        <li>Multi-modal micro-expression analysis, combining such as depth information, heart rate signal
                            etc.
                        </li>
                        <li>FME feature representation and computational analysis</li>
                        <li>Unified FME spot-and-recognize schemes</li>
                        <li>Deep learning techniques for FMEs detection and recognition</li>
                        <li>New objective classes for FMEs analysis</li>
                        <li>New FMEs datasets Facial expressions data synthesis</li>
                        <li>Psychology of FMEs research</li>
                        <li>Facial Action Unit (AU) detection and recognition</li>
                        <li>Emotion recognition using Aus</li>
                        <li>FME Applications</li>
                    </UL>
        </LI>
             </uL>
                <p><b>This workshop explores the intelligent analysis of personal emotions through facial expressions, <span style="color: #bb3e7f; "> with particular emphasis on micro-expression analysis to study hidden emotions</span>. Further, a focus on <span style="color:#bb3e7f; ">multi-modal approaches and novel generation techniques</span> will be encouraged.</b></p>
            </section>
            <section>
                <h2 class="major"><span>Submission</span></h2>
                <a name="submission"></a>
                <p> Please note: The submission deadline is at 11:59 p.m. of the stated deadline date <a
                        href="https://www.timeanddate.com/time/zones/aoe">Anywhere</a> on Earth.</p>
                <uL>
                    <Li><b>Submission Deadline:</b> <del>21st July 2023</del> 28th July 2023

                    <Li><b>Notification:</b> 30th July 2023
                    <Li><b>Camera-ready:</b> 06th August 2023
                    <Li><b>Submission guidelines:</b> ：

                        <ul>
                            <li>FME2023 workshop papers will go through a double-blind review process.
                            <li><b>Paper Format and page limit</b> : The template is the same as the one used for the
                                main conference (ACMMM23) track. Submitted papers (.pdf format) must use the ACM Article
                                Template <a href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
                                as used by regular ACMMM submissions. Please use the template in traditional <b>double-column
                                    format</b> to prepare your submissions. For example, word users may use Word Interim
                                Template, and latex users may use <b>sample-sigconf</b>
                                (\documentclass[sigconf,anonymous]{acmart}) template. The page limit would
                                be 8 pages.
                            <li><b>Submission system</b>: <a
                             href="https://easychair.org/conferences/?conf=fme2023">https://easychair.org/conferences/?conf=fme2023. </a>


                        </ul>
                </uL>


                <br/>
            </section>

        </div>
    </section>
    <footer id="footer">
        <!-- Copyright -->
        <div id="copyright">
            <ul class="menu">
                <li>GET IN TOUCH: lijt@psych.ac.cn</li>
                <li>&copy; MEGC2023. All rights reserved</li>
                <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
        </div>

    </footer>


</div>

<!-- Scripts -->
<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.dropotron.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
